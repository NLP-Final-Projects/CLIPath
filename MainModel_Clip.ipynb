{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">بسم الله الرحمن الرحیم</p>"
      ],
      "metadata": {
        "id": "fVlO1sEquuE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "dMXiYmf4qYAP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S5k14bHgOXN"
      },
      "outputs": [],
      "source": [
        "! pip install transformers\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import time\n",
        "import copy\n",
        "import PIL\n",
        "import torch\n",
        "import os\n",
        "import dill\n",
        "import clip\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from pkg_resources import packaging\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import CLIPModel, CLIPConfig, CLIPVisionModel, CLIPFeatureExtractor\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModel, TFAutoModel, AutoConfig\n",
        "from transformers import BertModel\n",
        "from transformers import TrainingArguments, Trainer, RobertaModel\n",
        "from transformers import default_data_collator\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cIE9-aNmFIr",
        "outputId": "00c6a741-1ec6-46ac-8e06-0cb85fcc82e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 1.13.1+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config and Hyper-parameter"
      ],
      "metadata": {
        "id": "fZhVptNPrJL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "TEST_SIZE = 0.1\n",
        "VAL_SIZE = 0.1\n",
        "BATCH_SIZE = 256\n",
        "EPOCH = 10\n",
        "LR = 1e-7\n",
        "EPS = 1e-9\n",
        "WEIGHT_DECAY = 0.1\n",
        "MAX_LR = 1e-2\n",
        "BASE_MODEL_PATH = '/content/drive/MyDrive/clip_trained_model/'"
      ],
      "metadata": {
        "id": "GZd56AYnpLaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prepare"
      ],
      "metadata": {
        "id": "zrW7NCekqzaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "hYJ7lWSinZhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP_Dataset(Dataset):\n",
        "    def __init__(self, list_image_path, list_text):\n",
        "        self.image_path = list_image_path\n",
        "        self.texts  = clip.tokenize(list_text)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = preprocess(Image.open(self.image_path[idx]))\n",
        "        text = self.texts[idx]\n",
        "        return image, text"
      ],
      "metadata": {
        "id": "zj-HnCq2nEY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(df, test_size=TEST_SIZE, shuffle=True, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=VAL_SIZE, shuffle=True, random_state=42)\n",
        "\n",
        "train_dataset = CLIP_Dataset(train_df['image_path'].tolist(), train_df['text'].tolist())\n",
        "val_dataset = CLIP_Dataset(val_df['image_path'].tolist(), val_df['text'].tolist())\n",
        "test_dataset = CLIP_Dataset(test_df['image_path'].tolist(), test_df['text'].tolist())\n",
        "\n",
        "dataloader = {\"train\": [], \"val\": [], \"test\": []}\n",
        "dataloader[\"train\"] = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "dataloader[\"val\"] = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "dataloader[\"test\"] = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "mJ75nElSnhoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for phase in [\"train\", \"val\", \"test\"]:\n",
        "    print(f\"=== {phase} ===\")\n",
        "    for item in dataloader[phase]:\n",
        "      print(f\"image shape: {item[0].shape}\")\n",
        "      print(f\"text shape: {item[0].shape}\")\n",
        "      break"
      ],
      "metadata": {
        "id": "CQKhdxhcoQ7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Loading"
      ],
      "metadata": {
        "id": "C3r2q2KBq2TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip.available_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pahTRHEmfnE",
        "outputId": "1a506708-667e-4580-a25f-1aca46c9966d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14',\n",
              " 'ViT-L/14@336px']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-L/14\", device=device, jit=False)\n",
        "model = model.float()"
      ],
      "metadata": {
        "id": "vNiC2sOXmqdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_num = [0, 0]\n",
        "freeze_layer_thr = 20\n",
        "\n",
        "for param in model.transformer.parameters():\n",
        "    layer_num[0]+=1\n",
        "for param in model.visual.parameters():\n",
        "    layer_num[1]+=1\n",
        "\n",
        "for i, param in enumerate(model.transformer.parameters()):\n",
        "    if freeze_layer_thr >= layer_num[0] - i:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "for i, param in enumerate(model.visual.parameters()):\n",
        "    if freeze_layer_thr >= layer_num[1] - i:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "GYOn9TePpVbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer, Loss, LR_Scheduler"
      ],
      "metadata": {
        "id": "EtoBnqeJrdwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), \n",
        "                             lr=LR,\n",
        "                             betas=(0.9,0.98),\n",
        "                             eps=EPS,\n",
        "                             weight_decay=WEIGHT_DECAY) \n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
        "                                                max_lr=MAX_LR, \n",
        "                                                steps_per_epoch=len(dataloader['train']), \n",
        "                                                epochs=EPOCH)"
      ],
      "metadata": {
        "id": "UkgIXG6TpR8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "idr59vGbrlnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "\n",
        "    for epoch in range(4, num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            total_loss = 0.0\n",
        "            num = 0\n",
        "\n",
        "            for batch in tqdm(dataloader[phase]):\n",
        "                optimizer.zero_grad()\n",
        "                images, texts = batch\n",
        "                images = images.to(device)\n",
        "                texts = texts.to(device)\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    logits_per_image, logits_per_text = model(images, texts)\n",
        "                    ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
        "                    batch_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        batch_loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                total_loss += batch_loss\n",
        "                num += 1\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = total_loss / num\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "            if phase == 'train':\n",
        "                torch.save(model.state_dict(), BASE_MODEL_PATH+f'clip_en_fi_ep{epoch}.pt')\n",
        "            \n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')"
      ],
      "metadata": {
        "id": "cXoGddbppbW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model=model, optimizer=optimizer, scheduler=scheduler, num_epochs=EPOCH)"
      ],
      "metadata": {
        "id": "heP9dWMmprt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "KXx_5rtMrndq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "total_loss = 0\n",
        "num = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(dataloader['test']):\n",
        "        images,texts = batch\n",
        "        images= images.to(device)\n",
        "        texts = texts.to(device)\n",
        "        logits_per_image, logits_per_text = model(images, texts)\n",
        "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
        "        total_loss += (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth))/2\n",
        "        num += 1\n",
        "total_loss = total_loss/num\n",
        "print(f'Test loss: {total_loss}')"
      ],
      "metadata": {
        "id": "IltE3hk0pzYf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}