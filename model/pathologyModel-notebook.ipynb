{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=justify dir=rtl>\n",
    "<h2>مدل پیشنهادی </h2>\n",
    "<p>\n",
    "در این نوتبوک کد مدل پیشنهادی قرار دارد. \n",
    "تفاوت این مدل با مدل قبلی در دو بخش است . اول این که ورودی که میگیرد متقاوت است با داده هایی که توسط مدل DINO فیلتر شده کار میکند . البته قصد داشتیم این فرآیند را با مدل دوم به صورت موازی پیش ببریم که به محدودیت gpu خوردیم. \n",
    "تفاوت دوم در مدل تکست انکودر است که در این بخش از تکست انکودر pathologyBert استفاده میکنیم که بر روی بیش از ۳۴۰ هزار گزارش پاتولوژی آموزش دیده است .همچنین از توکنایز مخصوص آن استفاده میکنیم\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import multiprocessing\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "from torch import nn\n",
    "from transformers import CLIPModel, CLIPConfig, CLIPVisionModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import default_data_collator\n",
    "from transformers import pipeline\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_MODEL = 'openai/clip-vit-base-patch32'\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 224\n",
    "MAX_LEN = 100\n",
    "MEAN = torch.tensor([0.48145466, 0.4578275, 0.40821073])\n",
    "STD = torch.tensor([0.26862954, 0.26130258, 0.27577711])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pathologyBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = pipeline('fill-mask', model='tsantos/PathologyBERT')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and Vision vision_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, AutoTokenizer, CLIPFeatureExtractor\n",
    "vision_preprocessor = CLIPFeatureExtractor.from_pretrained(IMAGE_MODEL)\n",
    "tokenizer = language_model.tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, image_paths: list, text: list, mode: str = 'train'):\n",
    "        self.image_paths = image_paths\n",
    "        self.tokens = tokenizer(text, padding='max_length',\n",
    "                                max_length=MAX_LEN, truncation=True)\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.augment = transforms.Compose([\n",
    "                transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=MEAN, std=STD)\n",
    "            ])\n",
    "        elif mode == 'test':\n",
    "            self.augment = transforms.Compose([\n",
    "                transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=MEAN, std=STD)\n",
    "            ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        token = self.tokens[idx]\n",
    "        return {'input_ids': token.ids, 'attention_mask': token.attention_mask,\n",
    "                'pixel_values': self.augment(Image.open(self.image_paths[idx]).convert('RGB'))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and set Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('E:/NLP/filter_data.csv')\n",
    "\n",
    "train,test = train_test_split(df,test_size=0.2, random_state=42)\n",
    "train_ds = CLIPDataset(image_paths=train.images.tolist(),\n",
    "                        text=train.captions.tolist(), mode='train')\n",
    "test_ds = CLIPDataset(image_paths=test.images.tolist(),\n",
    "                        text=test.captions.tolist(), mode='test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu():\n",
    "    torch.clear_autocast_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def optimal_workers():\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
    "    return optimal_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPVisionConfig, CLIPVisionModel\n",
    "configuration = CLIPVisionConfig()\n",
    "vision_encoder = CLIPVisionModel(configuration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltext = language_model.model.bert\n",
    "class BertPooler(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = torch.nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We take the hidden state corresponding to the first token\n",
    "        pooled_output = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(pooled_output)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "# Replace the original pooler with our custom pooler\n",
    "modeltext.pooler = BertPooler(modeltext.config)\n",
    "text_encoder= modeltext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_wraper_creator():\n",
    "    \"\"\"create a dummy CLIPModel to wrap text and vision encoders in order to use CLIPTrainer\"\"\"\n",
    "    config = {'num_hidden_layers': 0,\n",
    "              'max_position_embeddings': 0,\n",
    "              'vocab_size': 0,\n",
    "              'hidden_size': 1,\n",
    "              'patch_size': 1,\n",
    "              }\n",
    "    DUMMY_CONFIG = CLIPConfig(text_config_dict=config,\n",
    "                              vision_config_dict=config)\n",
    "    clip = CLIPModel(config=DUMMY_CONFIG)\n",
    "    # convert projectors to Identity\n",
    "    clip.text_projection = nn.Identity()\n",
    "    clip.visual_projection = nn.Identity()\n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = clip_wraper_creator()\n",
    "clip.text_model = text_encoder\n",
    "clip.vision_model = vision_encoder\n",
    "\n",
    "clip=clip.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs, return_loss=True)\n",
    "        return outputs[\"loss\"]\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys):\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        with torch.no_grad():\n",
    "            if 1:\n",
    "                with autocast():\n",
    "                    loss = self.compute_loss(model, inputs)\n",
    "            else:\n",
    "                loss = self.compute_loss(model, inputs)\n",
    "        return (loss, None, None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "        \"clip-fa\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=5000,\n",
    "        save_steps=5000,\n",
    "        logging_steps=5000,\n",
    "        learning_rate=3e-6,\n",
    "        weight_decay=0.003,\n",
    "        warmup_steps=100,\n",
    "        fp16=False,\n",
    "        prediction_loss_only=True,\n",
    "        gradient_accumulation_steps=1,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=300,\n",
    "        report_to='tensorboard'\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user01\\anaconda3\\envs\\BRACS2\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 67068\n",
      "  Num Epochs = 300\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 314400\n",
      "  Number of trainable parameters = 183481345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3977cdbcd6d48b49c3330a41db567fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/314400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args.dataloader_num_workers = optimal_workers()\n",
    "trainer = CLIPTrainer(clip, args,\n",
    "                        train_dataset=train_ds,\n",
    "                        eval_dataset=test_ds)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loade Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.load_state_dict(torch.load('E:/NLP/clip-fa_base_line/checkpoint-50000/pytorch_model.bin'))\n",
    "clip.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "counter=0\n",
    "\n",
    "for i in range(len(test)):\n",
    "  print(i)\n",
    "  image=Image.open(test.iloc[i]['images']).convert(\"RGB\")\n",
    "  editImage = vision_preprocessor(image)\n",
    "  proccess_image = editImage['pixel_values'][0].transpose(0, 1, 2)\n",
    "  image_input = torch.tensor(np.stack([proccess_image])).cuda()\n",
    "  \n",
    "  text_descriptions=[test.iloc[i]['captions']]\n",
    "\n",
    "  for txt in test.captions.tolist():\n",
    "    if txt not in text_descriptions:\n",
    "       text_descriptions.append(txt)\n",
    "\n",
    "  \n",
    "  tokens=tokenizer(text_descriptions,padding=True, truncation=True)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    image_features=clip.vision_model(image_input).pooler_output.float()\n",
    "    text_features=clip.text_model(torch.tensor(tokens.input_ids).cuda(),torch.tensor(tokens.attention_mask).cuda()).pooler_output.float()\n",
    "\n",
    "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "  text_probs = (1.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "  top_probs, top_labels = text_probs.cpu().topk(len(text_descriptions), dim=-1)\n",
    "  if top_labels[0][0].item() in [0]:\n",
    "    counter+=1\n",
    "\n",
    "\n",
    "print(\"Accuracy on Testset= \",counter/len(test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arabic-reshaper\n",
    "!pip install python-bidi\n",
    "import matplotlib.pyplot as plt\n",
    "from bidi.algorithm import get_display\n",
    "from arabic_reshaper import reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "clip.eval()\n",
    "\n",
    "# Open the image file and convert it to RGB format\n",
    "\n",
    "image = Image.open('E:/NLP/content/normalized/TCGA-EW-A1J2-01Z-00-DX1/3072/15860_43508.png').convert(\"RGB\")\n",
    "\n",
    "\n",
    "\n",
    "# Create a CLIPFeatureExtractor object and use it to pre-process the image\n",
    "vision_preprocessor = CLIPFeatureExtractor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "editImage = vision_preprocessor(image)\n",
    "\n",
    "# Transpose the tensor to the correct shape\n",
    "proccess_image = editImage['pixel_values'][0].transpose(0, 1, 2)\n",
    "\n",
    "# Convert the image to a tensor and move it to the GPU\n",
    "image_input = torch.tensor(np.stack([proccess_image])).cuda()\n",
    "text_descriptions=[        \n",
    "\n",
    "                   'Invasive lobular carcinoma in greatest linear dimension.'.lower(),\n",
    "                   'IVASIVE DUCTAL CARCINOMA, DUCTAL CARCINOMA IN SITU'.lower(),\n",
    "                   'LOBULAR CARCINOMA IN SITU. INFILTRATING LOBULAR CARCINOMA. LOBULAR CARCINOMA IN SITU'.lower()\n",
    "\n",
    "                   ]\n",
    "tokens=tokenizer(text_descriptions,padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "  image_features=clip.vision_model(image_input).pooler_output.float()\n",
    "  text_features=clip.text_model(torch.tensor(tokens.input_ids).cuda(),torch.tensor(tokens.attention_mask).cuda()).pooler_output.float()\n",
    "\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_probs = (1.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "top_probs, top_labels = text_probs.cpu().topk(len(text_descriptions), dim=-1)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "# Create a figure and an axis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(12, 8)\n",
    "\n",
    "persian_descriptions = [get_display(reshape(description)) for description in text_descriptions]\n",
    "# Use the `barh` method to create a horizontal bar plot\n",
    "ax1.barh(range(len(persian_descriptions)), top_probs[0])\n",
    "ax1.set_yticks(range(len(persian_descriptions)))\n",
    "ax1.set_yticklabels([persian_descriptions[i] for i in top_labels[0].numpy()])\n",
    "ax1.grid()\n",
    "ax2.imshow(image)\n",
    "ax2.axis(\"off\")\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot More example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "\n",
    "for i in range(len(test)):\n",
    "  counter+=1\n",
    "  k=random.randint(0, len(test)-1)\n",
    "  image=Image.open(test.iloc[k]['images']).convert(\"RGB\")\n",
    "  editImage = vision_preprocessor(image)\n",
    "  proccess_image = editImage['pixel_values'][0].transpose(0, 1, 2)\n",
    "  image_input = torch.tensor(np.stack([proccess_image])).cuda()\n",
    "  text_descriptions=[        \n",
    "                   test.iloc[k]['captions'].lower(),\n",
    "                   test.iloc[random.randint(0, len(test)-1)]['captions'].lower(),\n",
    "                   test.iloc[random.randint(0, len(test)-1)]['captions'].lower()\n",
    "                   ]\n",
    "  tokens=tokenizer(text_descriptions,padding=True, truncation=True)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    image_features=clip.vision_model(image_input).pooler_output.float()\n",
    "    text_features=clip.text_model(torch.tensor(tokens.input_ids).cuda(),torch.tensor(tokens.attention_mask).cuda()).pooler_output.float()\n",
    "\n",
    "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "  text_probs = (1.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "  top_probs, top_labels = text_probs.cpu().topk(len(text_descriptions), dim=-1)\n",
    "  if top_labels[0][0].item() ==0:\n",
    "    counter+=1\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.image as mpimg\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(12, 8)\n",
    "    persian_descriptions = [get_display(reshape(description)) for description in text_descriptions]\n",
    "\n",
    "    ax1.barh(range(len(persian_descriptions)), top_probs[0])\n",
    "    ax1.set_yticks(range(len(persian_descriptions)))\n",
    "    ax1.set_yticklabels([persian_descriptions[i] for i in top_labels[0].numpy()])\n",
    "    ax1.grid()\n",
    "    ax2.imshow(image)\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    if counter==10:\n",
    "      break\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BRACS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
