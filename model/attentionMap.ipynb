{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=justify dir=rtl>\n",
    "\n",
    "<p>\n",
    "در این بخش قرار است به کمک مدل DINO که قبلا روی حجم بسیاز زیادی داداه آموزش دیده کار کنیم و به ازای هر تصویر بردار attention آنها را ترسیم میکنیم و محاسبه میکنیم که اگر تعداد پیکسل هایی که میزان attention در آنها از یک حد آستانه ای  بیشتر باشد از یک مقدار مشخص بالا تر بود ،  آن تصویر را به عنوان تصویر مناسب در نظر میگیریم\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=justify dir=rtl>\n",
    "<h2>بارگذاری داده</h2>\n",
    "<p>\n",
    "اگر بر روی کولب کار میکنیم در ابتدا لازم است یک سری کتابحانه نصب گردند\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_lists=[]\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import cv2\n",
    "import random\n",
    "import colorsys\n",
    "import requests\n",
    "from io import BytesIO\n",
    "z=0\n",
    "import skimage.io\n",
    "from skimage.measure import find_contours\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms as pth_transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import utils\n",
    "import vision_transformer as vits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=justify dir=rtl>\n",
    "<h2>محاسبه میزان attention و فیلتر تصاویر</h2>\n",
    "<p>\n",
    "همانند بخش آموزش مدل DINO در اینجا نیز یک سری آرگومان داریم که با قرار دادن مقدار default با مقداری که مدنظرمان است آنها را پر میکنیم.\n",
    "</p>\n",
    "<ul>\n",
    "<li>\n",
    "arch: با این آرگومان مشخص میکنیم که بک بون مدل چه چیز است.\n",
    "</li>\n",
    "<li>\n",
    "patch_size : سایز وصله های ورودی مدل را مشخص میکنیم\n",
    "\n",
    "</li>\n",
    "<li>\n",
    "pretrained_weights : مشخص کننده مسیر وزن های مدل أموزش دیده\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser('Visualize Self-Attention maps')\n",
    "    parser.add_argument('--arch', default='vit_small', type=str,\n",
    "        choices=['vit_tiny', 'vit_small', 'vit_base'], help='Architecture (support only ViT atm).')\n",
    "    parser.add_argument('--patch_size', default=16, type=int, help='Patch resolution of the model.')\n",
    "    parser.add_argument('--pretrained_weights', default='vits_tcga_brca_dino.pt', type=str,\n",
    "        help=\"Path to pretrained weights to load.\")\n",
    "    parser.add_argument(\"--checkpoint_key\", default=\"teacher\", type=str,\n",
    "        help='Key to use in the checkpoint (example: \"teacher\")')\n",
    "    parser.add_argument(\"--image_path\", default=\"\", type=str, help=\"\")\n",
    "    parser.add_argument(\"--image_size\", default=(224, 224), type=int, nargs=\"+\", help=\"Resize image.\")\n",
    "    parser.add_argument('--output_dir', default='/content/', help='Path where to save visualizations.')\n",
    "    parser.add_argument(\"--threshold\", type=float, default=None, help=\"\"\"We visualize masks\n",
    "        obtained by thresholding the self-attention maps to keep xx% of the mass.\"\"\")\n",
    "    args = parser.parse_args(\"\")\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    # build model\n",
    "    model = vits.__dict__[args.arch](patch_size=args.patch_size, num_classes=0)\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    if os.path.isfile(args.pretrained_weights):\n",
    "        state_dict = torch.load(args.pretrained_weights, map_location=\"cpu\")\n",
    "        if args.checkpoint_key is not None and args.checkpoint_key in state_dict:\n",
    "            print(f\"Take key {args.checkpoint_key} in provided checkpoint dict\")\n",
    "            state_dict = state_dict[args.checkpoint_key]\n",
    "        # remove `module.` prefix\n",
    "        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        # remove `backbone.` prefix induced by multicrop wrapper\n",
    "        state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "        msg = model.load_state_dict(state_dict, strict=False)\n",
    "        print('Pretrained weights found at {} and loaded with msg: {}'.format(args.pretrained_weights, msg))\n",
    "    else:\n",
    "        print(\"Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\")\n",
    "        url = None\n",
    "        if args.arch == \"vit_small\" and args.patch_size == 16:\n",
    "            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
    "        elif args.arch == \"vit_small\" and args.patch_size == 8:\n",
    "            url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"  # model used for visualizations in our paper\n",
    "        elif args.arch == \"vit_base\" and args.patch_size == 16:\n",
    "            url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n",
    "        elif args.arch == \"vit_base\" and args.patch_size == 8:\n",
    "            url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n",
    "        if url is not None:\n",
    "            print(\"Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\")\n",
    "            state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
    "            model.load_state_dict(state_dict, strict=True)\n",
    "        else:\n",
    "            print(\"There is no reference weights available for this model => We use random weights.\")\n",
    "\n",
    "    # open image\n",
    "    import os\n",
    "    path = 'E:/NLP/content/normalized/'\n",
    "    folders = os.listdir(path)\n",
    "    for folder in folders:\n",
    "      images=path+folder+'/1024/'\n",
    "      images = os.listdir(images)\n",
    "      # print(os.listdir(images))\n",
    "      # break\n",
    "      for image in images:\n",
    "        image_path = path+folder+'/1024/'+image\n",
    "        if image_path is None:\n",
    "            # user has not specified any image - we use our own image\n",
    "            print(\"Please use the `--image_path` argument to indicate the path of the image you wish to visualize.\")\n",
    "            print(\"Since no image path have been provided, we take the first image in our paper.\")\n",
    "            response = requests.get(\"https://dl.fbaipublicfiles.com/dino/img.png\")\n",
    "            img = Image.open(image_path)\n",
    "            img = img.convert('RGB')\n",
    "        elif os.path.isfile(image_path):\n",
    "            with open(image_path, 'rb') as f:\n",
    "                img = Image.open(f)\n",
    "                img = img.convert('RGB')\n",
    "        else:\n",
    "            print(f\"Provided image path {image_path} is non valid.\")\n",
    "            sys.exit(1)\n",
    "        transform = pth_transforms.Compose([\n",
    "            pth_transforms.Resize(args.image_size),\n",
    "            pth_transforms.ToTensor(),\n",
    "            pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "        img = transform(img)\n",
    "\n",
    "        # make the image divisible by the patch size\n",
    "        w, h = img.shape[1] - img.shape[1] % args.patch_size, img.shape[2] - img.shape[2] % args.patch_size\n",
    "        img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "        w_featmap = img.shape[-2] // args.patch_size\n",
    "        h_featmap = img.shape[-1] // args.patch_size\n",
    "\n",
    "        attentions = model.get_last_selfattention(img.to(device))\n",
    "\n",
    "        nh = attentions.shape[1] # number of head\n",
    "\n",
    "        # we keep only the output patch attention\n",
    "        attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "\n",
    "        if args.threshold is not None:\n",
    "            # we keep only a certain percentage of the mass\n",
    "            val, idx = torch.sort(attentions)\n",
    "            val /= torch.sum(val, dim=1, keepdim=True)\n",
    "            cumval = torch.cumsum(val, dim=1)\n",
    "            th_attn = cumval > (1 - args.threshold)\n",
    "            idx2 = torch.argsort(idx)\n",
    "            for head in range(nh):\n",
    "                th_attn[head] = th_attn[head][idx2[head]]\n",
    "            th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()\n",
    "            # interpolate\n",
    "            th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=args.patch_size, mode=\"nearest\")[0].cpu().numpy()\n",
    "\n",
    "        attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "        attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=args.patch_size, mode=\"nearest\")[0].cpu().numpy()\n",
    "        count1 = np.count_nonzero(attentions[0] > 0.01)\n",
    "        count2 = np.count_nonzero(attentions[1] > 0.01)\n",
    "        count3 = np.count_nonzero(attentions[2] > 0.01)\n",
    "\n",
    "\n",
    "        # calculate the percentage of values greater than 0.01\n",
    "        percentage1 = count1 / attentions[0].size\n",
    "        percentage2 = count2 / attentions[1].size\n",
    "        percentage3 = count3 / attentions[2].size\n",
    "\n",
    "        # check if 80% of the values are greater than 0.01\n",
    "        if percentage1 >= 0.1 and percentage2>=0.1 and percentage3 >=0.1:\n",
    "        # if percentage1 >= 0.1 :\n",
    "          \n",
    "        #   print(image_path)\n",
    "          image_lists.append(image_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## captions for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "folder_path = \"E:/NLP/Finalized_samples 2\"  # replace with the path to your folder\n",
    "\n",
    "# get a list of all txt files in the folder\n",
    "txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "\n",
    "# create an empty dictionary to store file contents\n",
    "file_contents = {}\n",
    "\n",
    "# loop through each txt file\n",
    "for txt_file in txt_files:\n",
    "    # get the filename without the path and extension\n",
    "    file_name = os.path.splitext(os.path.basename(txt_file))[0]\n",
    "    \n",
    "    # read the file content and store it in the dictionary\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        content = f.read()\n",
    "        file_contents[file_name] = content\n",
    "\n",
    "captions=[]\n",
    "for image in image_lists:\n",
    "    captions.append(file_contents[image[26:38]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=justify dir=rtl>\n",
    "\n",
    "<p>\n",
    "در آخر تصاویر انتخابی را در قالب یک فایل csv ذخیره میکنیم تا در مراحل بعدی بتوانیم از آن استفاده کنیم\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame()\n",
    "\n",
    "df['images']=image_lists\n",
    "df['captions']=captions\n",
    "df.to_csv('filter_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BRACS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
